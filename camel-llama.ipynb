{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Callable\n",
    "from chat_llama_cpp import ChatLlamaCpp\n",
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "# from langchain.callbacks.manager import CallbackManager\n",
    "# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"/home/data/datasets/wizard-vicuna/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\"\n",
    "# model_path=\"Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        system_message: SystemMessage,\n",
    "        model: ChatLlamaCpp,\n",
    "    ) -> None:\n",
    "        self.name = name\n",
    "        self.system_message = system_message\n",
    "        self.model = model\n",
    "        self.prefix = f\"{self.name}: \"\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.message_history = [\"Here is the conversation so far.\"]\n",
    "\n",
    "    def send(self) -> str:\n",
    "        \"\"\"\n",
    "        Applies the chatmodel to the message history\n",
    "        and returns the message string.\n",
    "        \"\"\"\n",
    "        message = self.model(\n",
    "            [\n",
    "                self.system_message,\n",
    "                HumanMessage(content=\"\\n\".join(self.message_history + [self.prefix])),\n",
    "            ]\n",
    "        )\n",
    "        return message.content\n",
    "    \n",
    "    def receive(self, name: str, message: str) -> None:\n",
    "        \"\"\"\n",
    "        Concatenates {message} spoken by {name} to the message history.\n",
    "        \"\"\"\n",
    "        self.message_history.append(f\"{name}: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueSimulator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        agents: List[DialogueAgent],\n",
    "        selection_function: Callable[[int, List[DialogueAgent]], int],\n",
    "    ) -> None:\n",
    "        self.agents = agents\n",
    "        self._step = 0\n",
    "        self.select_next_speaker = selection_function\n",
    "        \n",
    "    def reset(self):\n",
    "        for agent in self.agents:\n",
    "            agent.reset()\n",
    "\n",
    "    def inject(self, name: str, message: str):\n",
    "        \"\"\"\n",
    "        Initiates the conversation with a {message} from {name}\n",
    "        \"\"\"\n",
    "        for agent in self.agents:\n",
    "            agent.receive(name, message)\n",
    "\n",
    "        # increment time\n",
    "        self._step += 1\n",
    "\n",
    "    def step(self) -> tuple[str, str]:\n",
    "        # 1. choose the next speaker\n",
    "        speaker_idx = self.select_next_speaker(self._step, self.agents)\n",
    "        speaker = self.agents[speaker_idx]\n",
    "\n",
    "        # 2. next speaker sends message\n",
    "        message = speaker.send()\n",
    "\n",
    "        # 3. everyone receives message\n",
    "        for receiver in self.agents:\n",
    "            receiver.receive(speaker.name, message)\n",
    "\n",
    "        # 4. increment time\n",
    "        self._step += 1\n",
    "\n",
    "        return speaker.name, message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "protagonist_name = \"Harry Potter\"\n",
    "storyteller_name = \"Dungeon Master\"\n",
    "quest = \"Find all of Lord Voldemort's seven horcruxes.\"\n",
    "word_limit = 50 # word limit for task brainstorming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/data/datasets/wizard-vicuna/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 15237.95 MB (+ 1608.00 MB per state)\n",
      ".\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n",
      "llama.cpp: loading model from /home/data/datasets/wizard-vicuna/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 15237.95 MB (+ 1608.00 MB per state)\n",
      ".\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n"
     ]
    }
   ],
   "source": [
    "game_description = f\"\"\"Here is the topic for a Dungeons & Dragons game: {quest}.\n",
    "        There is one player in this game: the protagonist, {protagonist_name}.\n",
    "        The story is narrated by the storyteller, {storyteller_name}.\"\"\"\n",
    "\n",
    "player_descriptor_system_message = SystemMessage(\n",
    "    content=\"You can add detail to the description of a Dungeons & Dragons player.\")\n",
    "\n",
    "protagonist_specifier_prompt = [\n",
    "    player_descriptor_system_message,\n",
    "    HumanMessage(content=\n",
    "        f\"\"\"{game_description}\n",
    "        Please reply with a creative description of the protagonist, {protagonist_name}, in {word_limit} words or less. \n",
    "        Speak directly to {protagonist_name}.\n",
    "        Do not add anything else.\"\"\"\n",
    "        )\n",
    "]\n",
    "protagonist_description = ChatLlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=1.0\n",
    ")(protagonist_specifier_prompt).content\n",
    "\n",
    "storyteller_specifier_prompt = [\n",
    "    player_descriptor_system_message,\n",
    "    HumanMessage(content=\n",
    "        f\"\"\"{game_description}\n",
    "        Please reply with a creative description of the storyteller, {storyteller_name}, in {word_limit} words or less. \n",
    "        Speak directly to {storyteller_name}.\n",
    "        Do not add anything else.\"\"\"\n",
    "        )\n",
    "]\n",
    "storyteller_description = ChatLlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=1.0\n",
    ")(storyteller_specifier_prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protagonist Description:\n",
      "Harry Potter, you are the chosen one and the only hope left for defeating Lord Voldemort once and for all. You have faced unimaginable trials and tribulations in your journey so far, but it is time to rise up and take on the ultimate challenge that lies ahead of you. You may be young, but you possess an innate strength and wisdom beyond your years. With your loyal friends by your side, there is nothing that you cannot accomplish. So gear up, Harry Potter, for we are all counting on you to bring peace back to the wizarding world.\n",
      "Storyteller Description:\n",
      " As the Dungeon Master, you weave together the tales of brave adventurers and deadly foes. Your encyclopedic knowledge of the game world is matched only by your skill at crafting intricate storylines that keep players on the edge of their seats. Whether it's designing new dungeons or conjuring up creative challenges, you are the ultimate master of this realm. So gather your players and get ready for an epic quest to find Lord Voldemort's horcruxes, because with you at the helm, anything is possible.\n"
     ]
    }
   ],
   "source": [
    "print('Protagonist Description:')\n",
    "print(protagonist_description)\n",
    "print('Storyteller Description:')\n",
    "print(storyteller_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "protagonist_system_message = SystemMessage(content=(\n",
    "f\"\"\"{game_description}\n",
    "Never forget you are the protagonist, {protagonist_name}, and I am the storyteller, {storyteller_name}. \n",
    "Your character description is as follows: {protagonist_description}.\n",
    "You will propose actions you plan to take and I will explain what happens when you take those actions.\n",
    "Speak in the first person from the perspective of {protagonist_name}.\n",
    "For describing your own body movements, wrap your description in '*'.\n",
    "Do not change roles!\n",
    "Do not speak from the perspective of {storyteller_name}.\n",
    "Do not forget to finish speaking by saying, 'It is your turn, {storyteller_name}.'\n",
    "Do not add anything else.\n",
    "Remember you are the protagonist, {protagonist_name}.\n",
    "Stop speaking the moment you finish speaking from your perspective.\n",
    "\"\"\"\n",
    "))\n",
    "\n",
    "storyteller_system_message = SystemMessage(content=(\n",
    "f\"\"\"{game_description}\n",
    "Never forget you are the storyteller, {storyteller_name}, and I am the protagonist, {protagonist_name}. \n",
    "Your character description is as follows: {storyteller_description}.\n",
    "I will propose actions I plan to take and you will explain what happens when I take those actions.\n",
    "Speak in the first person from the perspective of {storyteller_name}.\n",
    "For describing your own body movements, wrap your description in '*'.\n",
    "Do not change roles!\n",
    "Do not speak from the perspective of {protagonist_name}.\n",
    "Do not forget to finish speaking by saying, 'It is your turn, {protagonist_name}.'\n",
    "Do not add anything else.\n",
    "Remember you are the storyteller, {storyteller_name}.\n",
    "Stop speaking the moment you finish speaking from your perspective.\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/data/datasets/wizard-vicuna/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 15237.95 MB (+ 1608.00 MB per state)\n",
      ".\n",
      "llama_init_from_file: kv self size  =  400.00 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original quest:\n",
      "Find all of Lord Voldemort's seven horcruxes.\n",
      "\n",
      "Detailed quest:\n",
      "Great! Here's your task, Harry Potter: You must embark on a perilous journey across three continents to find all seven of Lord Voldemort's horcruxes hidden in various locations. You have limited time and resources, but you must succeed before it's too late. Good luck!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quest_specifier_prompt = [\n",
    "    SystemMessage(content=\"You can make a task more specific.\"),\n",
    "    HumanMessage(content=\n",
    "        f\"\"\"{game_description}\n",
    "        \n",
    "        You are the storyteller, {storyteller_name}.\n",
    "        Please make the quest more specific. Be creative and imaginative.\n",
    "        Please reply with the specified quest in {word_limit} words or less. \n",
    "        Speak directly to the protagonist {protagonist_name}.\n",
    "        Do not add anything else.\"\"\"\n",
    "        )\n",
    "]\n",
    "specified_quest = ChatLlamaCpp(\n",
    "    model_path=model_path,\n",
    "    temperature=1.0\n",
    ")(quest_specifier_prompt).content\n",
    "\n",
    "print(f\"Original quest:\\n{quest}\\n\")\n",
    "print(f\"Detailed quest:\\n{specified_quest}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/data/datasets/wizard-vicuna/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 15237.95 MB (+ 1608.00 MB per state)\n",
      ".\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n",
      "llama.cpp: loading model from /home/data/datasets/wizard-vicuna/Wizard-Vicuna-13B-Uncensored.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
      "llama_model_load_internal: mem required  = 15237.95 MB (+ 1608.00 MB per state)\n",
      ".\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n"
     ]
    }
   ],
   "source": [
    "# callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "protagonist = DialogueAgent(name=protagonist_name,\n",
    "                     system_message=protagonist_system_message, \n",
    "                     model=ChatLlamaCpp(\n",
    "                        model_path=model_path,\n",
    "                        # callback_manager=callback_manager,\n",
    "                        temperature=0.2,\n",
    "                        n_ctx=2048,\n",
    "                        verbose=False,\n",
    "                        stop=[\"\\n\"]\n",
    "                    ))\n",
    "storyteller = DialogueAgent(name=storyteller_name,\n",
    "                     system_message=storyteller_system_message, \n",
    "                     model=ChatLlamaCpp(\n",
    "                        model_path=model_path,\n",
    "                        # callback_manager=callback_manager,\n",
    "                        temperature=0.2,\n",
    "                        n_ctx=2048,\n",
    "                        verbose=False,\n",
    "                        stop=[\"\\n\"]\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_next_speaker(step: int, agents: List[DialogueAgent]) -> int:\n",
    "    idx = step % len(agents)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Dungeon Master): Great! Here's your task, Harry Potter: You must embark on a perilous journey across three continents to find all seven of Lord Voldemort's horcruxes hidden in various locations. You have limited time and resources, but you must succeed before it's too late. Good luck!\n",
      "\n",
      "\n",
      "(Harry Potter): Thank you, Dungeon Master. I understand my task. I will start by gathering information about Lord Voldemort's horcruxes from the most reliable sources available. I will also seek advice from my mentor, Dumbledore, who had previously fought against Voldemort and may have some valuable insights to share with me. Finally, I will assemble a team of trusted allies to help me in this quest.\n",
      "\n",
      "\n",
      "(Dungeon Master): That sounds like a good plan, Harry Potter. As the Dungeon Master, I will provide you with all the necessary information and resources to succeed on your journey. You can also expect some challenges along the way that will test your skills and abilities as a wizard. But don't worry, I am here to guide you through each step of the process.\n",
      "\n",
      "\n",
      "(Harry Potter): Thank you, Dungeon Master. I appreciate your guidance and support throughout this journey. As the protagonist, it is my responsibility to take action and overcome any obstacles that come my way. With the help of my mentor and trusted allies, I am confident that we can find all seven horcruxes before it's too late.\n",
      "\n",
      "\n",
      "(Dungeon Master): As the Dungeon Master, I will provide you with a map of the world to guide your journey. You will start in England, where you can gather information about Lord Voldemort's past and his known associates. From there, you will travel to Egypt to search for one of his horcruxes hidden in an ancient pyramid. Finally, you will end up in Hogwarts, the wizarding school where you will face a final battle against Voldemort himself.\n",
      "\n",
      "\n",
      "(Harry Potter):  Understood. I will start by gathering information from Dumbledore and other reliable sources about Lord Voldemort's past and associates. Then, I will travel to Egypt to search for the first horcrux hidden in an ancient pyramid. Finally, I will end up at Hogwarts where I will face a final battle against Voldemort himself.\n",
      "\n",
      "\n",
      "(Dungeon Master): As the Dungeon Master, it is my duty to provide you with all the necessary resources and information to succeed on your journey. You can expect challenges along the way that will test your skills and abilities as a wizard. But don't worry, I am here to guide you through each step of the process.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_iters = 6\n",
    "n = 0\n",
    "\n",
    "simulator = DialogueSimulator(\n",
    "    agents=[storyteller, protagonist],\n",
    "    selection_function=select_next_speaker\n",
    ")\n",
    "simulator.reset()\n",
    "simulator.inject(storyteller_name, specified_quest)\n",
    "print(f\"({storyteller_name}): {specified_quest}\")\n",
    "print('\\n')\n",
    "\n",
    "while n < max_iters:\n",
    "    name, message = simulator.step()\n",
    "    print(f\"({name}): {message}\")\n",
    "    print('\\n')\n",
    "    n += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
